{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#Наивная-модель-(константное-значение)\" data-toc-modified-id=\"Наивная-модель-(константное-значение)-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Наивная модель (константное значение)</a></span></li><li><span><a href=\"#Логистическая-регрессия\" data-toc-modified-id=\"Логистическая-регрессия-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Логистическая регрессия</a></span></li><li><span><a href=\"#Catboost-classifier\" data-toc-modified-id=\"Catboost-classifier-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Catboost classifier</a></span></li><li><span><a href=\"#Случайный-лес\" data-toc-modified-id=\"Случайный-лес-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Случайный лес</a></span></li><li><span><a href=\"#Полносвязная-нейронная-сеть\" data-toc-modified-id=\"Полносвязная-нейронная-сеть-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Полносвязная нейронная сеть</a></span></li></ul></li><li><span><a href=\"#Тестирование-лучшей-модели\" data-toc-modified-id=\"Тестирование-лучшей-модели-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Тестирование лучшей модели</a></span><ul class=\"toc-item\"><li><span><a href=\"#Логистическая-регрессия\" data-toc-modified-id=\"Логистическая-регрессия-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Логистическая регрессия</a></span></li></ul></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Итоги\" data-toc-modified-id=\"Итоги-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Итоги</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп» с BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, AutoModel\n",
    "from tqdm import notebook\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 159292 entries, 0 to 159450\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159292 non-null  object\n",
      " 1   toxic   159292 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31055</th>\n",
       "      <td>Sometime back, I just happened to log on to ww...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102929</th>\n",
       "      <td>\"\\n\\nThe latest edit is much better, don't mak...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67385</th>\n",
       "      <td>\" October 2007 (UTC)\\n\\nI would think you'd be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81167</th>\n",
       "      <td>Thanks for the tip on the currency translation...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90182</th>\n",
       "      <td>I would argue that if content on the Con in co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1860</th>\n",
       "      <td>\"=Reliable sources===\\nCheating:\\n\"\"Barry Bond...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125422</th>\n",
       "      <td>WTF=\\n\\nHow The Fuck Does This Person Merit A ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149142</th>\n",
       "      <td>cajuns, acadians\\nCajuns, acadians, louisianan...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89784</th>\n",
       "      <td>Hi - I dropped a pin in Google Maps at the cer...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64323</th>\n",
       "      <td>Re removal of accessdate= for urls books \\n\\nT...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic\n",
       "31055   Sometime back, I just happened to log on to ww...      0\n",
       "102929  \"\\n\\nThe latest edit is much better, don't mak...      0\n",
       "67385   \" October 2007 (UTC)\\n\\nI would think you'd be...      0\n",
       "81167   Thanks for the tip on the currency translation...      0\n",
       "90182   I would argue that if content on the Con in co...      0\n",
       "1860    \"=Reliable sources===\\nCheating:\\n\"\"Barry Bond...      1\n",
       "125422  WTF=\\n\\nHow The Fuck Does This Person Merit A ...      1\n",
       "149142  cajuns, acadians\\nCajuns, acadians, louisianan...      0\n",
       "89784   Hi - I dropped a pin in Google Maps at the cer...      0\n",
       "64323   Re removal of accessdate= for urls books \\n\\nT...      0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    df = pd.read_csv('/datasets/toxic_comments.csv', index_col=0)\n",
    "except:\n",
    "    df = pd.read_csv('./toxic_comments.csv', index_col=0)\n",
    "df.info()\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дубликаты отсутствуют"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 2 artists>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEJCAYAAACZjSCSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAb4ElEQVR4nO3debQcVbn38e8vBwLBDFxIUEMSToCgAjK4ArmK1+EFNSgQXpllFgiggMoFAYeICMjMwpco0w2TCnLBsKJEEETxKgJJLnMQDCGYhDkEkyBTkuf9o3a7ik6fcyrJqT45Xb/PWr1O17T7qe4+/dTeu2qXIgIzM6uuPj0dgJmZ9SwnAjOzinMiMDOrOCcCM7OKcyIwM6s4JwIzs4pzIjAzqzgngl5I0hxJb0haIulFSddI6t/TcZlZ7+RE0HvtHhH9gY8Ao4Hv9HA8ZtZLORH0chExH/gNsDWApMMlPSFpsaTZko7Ory9pnKSHJC2S9LSksWn+HyS9mWoZS1KNY05uuzmSTpM0U9JCSVdLWje3fLdU7muS7pW0Td3r/lTS27my5+WWrSPpAkl/TzWcyyT1yy1vlxS52JZJOjIt6yPp1LQvCyTdJGmDuu3Wqovj9PT8U3Vx7JvWPzI378vp/Vwo6Q5JmzT6HLp6rTT9QUl3SnpV0pOS9q0r43RJ76R9fD1fnqShkm6R9LKkZySd0MF2r0maLGlA7v35jqRnJb0k6TpJgzp4X5dIOjf3elNSrLMkHdVov9O610g6Mz3fMH1Hji34+e0o6S8p7uclXSqpb27brXLv2YuSvpXmt0n6VvrcF0uaIWl4WnaJpLnKvuMzJP1HR7Fbxomgl0tf/s8DD6ZZLwG7AQOBw4GLJX0krbsjcB1wMrA+8AlgTq644yKif6pp7N7g5Q4EPgdsBmxBqoVI2h6YBBwNbAhcDkyRtE4+VOCsVPaudeWek8rbDtgc2BiYkFte+54OStv/T27Z8cCewCeBocBCYGKD2DslaW3gB8DzuXnjgG8BXwSGpNe9YWXLTmW9B7gT+DmwEbA/8GNJW+ZW6wPcmPZxq9y2fYBfAQ+TvTc7A1+X9Lnctr9I240ARgKHpvmHpcengU2B/sCldeGtX/vcI+KUNO9GYB7Ze7o3cLak/9PFPvYnOyj5eUT8pG6/oPHntwz4BjAY+Gjat6+k8gYAdwG3pzg2B36XtjsROIDsuz8Q+DLwz7RsGtl3aQOy9/u/lTtosRU5EfRet0p6DfgTcA9wNkBE3BYRT0fmHuC3QO2I6AhgUkTcGRHLI2J+RPx1JV7z0oiYGxGvAmeR/SMCjAcuj4j7I2JZRFwLvAX8e27bfsDb9QVKUtr+GxHxakQsTvuyf261vsDyiFjWIKZjgG9HxLyIeAs4Hdg7f2Re0NHA/cBTdWX/MCKeiIilKa7tOqoVdGE3YE5EXB0RSyPiQeAWYJ/cOn1p8B4BOwBDIuKMiHg7ImYDV/Lu96imjez/ekGaPhC4KCJmR8QS4DRg/87en3RwsRNwSkS8GREPAVcBh3Syf+sAtwJPRMSZdcs6/PwiYkZE3JfekzlkBxGfTIt3A16IiAtTHIsj4v607EjgOxHxZPquPxwRC1KZP42IBanMC1NsH+gk9spb2X8WW3PsGRF31c+UtCvwPbIj7D7AesCjafFwYOpqvObc3PNnyY7SADYBDpV0fG5539xygPcBLzcoc0iKcUaWE4Cs9tCWW2cDsiP9RjYBJktanpu3DHhvbvqVXNnrkZLmv14sO/L8JlnCvLau7EskXZhfneyo/NkO4unIJsCYlLxr1gKuz013tJ+bAEPrtm3j3UfW+0rajeyIfxpZDQKyzyAf67PpdfPvT72hQC0p57cb3ck2XyWrsXxMUr+IeCO3rMPPT9IWwEWp7PVSbDPS4uHA0x28XofLJJ1EdtAzFAiyGsPgTmKvPNcIWkhqirkFuAB4b0SsT/bDX/sVnEvWrLOqhueejwCey5V7VkSsn3usFxE3pLjWJuvDeLhBma8AbwBb5batNSHUbMG7j9Tz5gK71r32uqnvpGZwbRlwU4MyTgZuioj6H/e5wNF1ZfeLiHs7iKUzc4F76srqHxHH5tbpaD/nAs/UbTsgIj6fW+emtH+1xF9LXs+RJZKaEcBS4MVOYn0O2KDWz5Dbbn4H6wPcS5ZIp5HVFvM6+/x+AvwVGBURA8ma4vLf10072K7hdzn1B3wT2Bf4t/Se/CNXpjXgRNBa+pJVg18GlqbawWdzy/8LOFzSzqkTcWNJH1yJ8r8qaZiyzthvA79I868EjpE0Rpn3SPpC7ofkcOAFYHp9gRGxPG1/saSNAFJcn0vPhwNfI2t2aOQy4Kxac42kIaltv6gBKb76H69a2adJ2iqVPUjSPg3WK+LXwBaSDpa0dnrsIOlD6T0bR3ZU/JsG2z4ALJZ0iqR+qaN0a0k7NFh3OdlR8JA0fQPwDUkjUxv+2WT9CUs7CjQi5pL9sP9Q0rrKOv6PAH7ayf7dl8o8AThA0keh0Oc3AFgELEnfxXxi/DXwfklfV3ZCwQBJY9Kyq4AfSBqV3r9tJG2YyltK9j+wlqQJZDUC64QTQQtJVfkTyI56FwJfAqbklj9A6kAmO0q6h3cfLXbl52R9DrPJquVnpnKnA0eRdUIuBGaRdVAi6UCydt+RZD9mS8h+7IZKuiyVe0ra5j5Ji8g6CGttuncAf0gxN3JJ2sffSloM3AeM6WDdRgYCP4qIFZouImIycC5wY4rrMVbs6K43R9I8ZWcj/V/gREn7pM/ms2Tt+s+RJcZzyRL3WLL38sD0I1wfxzKy9vLtgGfIalFXAYNyq+2X3tsFwJZkR9aQdeJfD/wxbfsmWQd7Vw4A2lOsk4HvNWqKbBDrK6n8SamG2tXndxLZ93Qx2QFB7eCi9n3+DNmJCy8AfyPr9IasOekmsu/jIrKDnH7p9W4nq4E8m/Z3hffU3k2+MY0VoexU0iOL/BjUbXcY0B4Rp9fNHwacGRGHdVOIayRlp4/OiYhrejgUsw65s9jK9jrZEVu9pcCrTY6lJ8wmO5o1W2O5RmCFrGqNwMzWfE4EZmYV585iM7OK63V9BIMHD4729vaeDsPMrFeZMWPGKxExpNGyXpcI2tvbmT59hdPRzcysE5I6vBreTUNmZhXnRGBmVnFOBGZmFedEYGZWcU4EZmYV50RgZlZxTgRmZhXnRGBmVnFOBGZmFdfrrixeHe2n3tbTIdgabM45X+jpEMx6hGsEZmYV50RgZlZxTgRmZhXnRGBmVnFOBGZmFedEYGZWcU4EZmYV50RgZlZxTgRmZhXnRGBmVnFOBGZmFedEYGZWcU4EZmYV50RgZlZxTgRmZhXnRGBmVnFOBGZmFedEYGZWcaUmAkljJT0paZakUxssHyHp95IelPSIpM+XGY+Zma2otEQgqQ2YCOwKbAkcIGnLutW+A9wUEdsD+wM/LiseMzNrrMwawY7ArIiYHRFvAzcC4+rWCWBgej4IeK7EeMzMrIEyE8HGwNzc9Lw0L+904CBJ84CpwPGNCpI0XtJ0SdNffvnlMmI1M6usnu4sPgC4JiKGAZ8Hrpe0QkwRcUVEjI6I0UOGDGl6kGZmrazMRDAfGJ6bHpbm5R0B3AQQEX8B1gUGlxiTmZnVKTMRTANGSRopqS9ZZ/CUunX+DuwMIOlDZInAbT9mZk1UWiKIiKXAccAdwBNkZwc9LukMSXuk1f4TOErSw8ANwGEREWXFZGZmK1qrqxUk/Tvw/4APAX2BNuD1iBjY6YZAREwl6wTOz5uQez4T2GklYzYzs25UpEZwKVmn7t+AfsCRZNcHmJlZCyjUNBQRs4C2iFgWEVcDY8sNy8zMmqXLpiHgn6mz9yFJ5wHP0/OnnZqZWTcp8oN+cFrvOOB1slNC9yozKDMza54uawQR8Wx6+qakHwLrRMTicsMyM7Nm6bJGIOkbaXiHQ4CngL9JOrn80MzMrBmK9BF8lexisLuBduBNYDpwfnlhmZlZsxRJBIsiYrqkpyPiVQBJb5Ycl5mZNUmRRLCppCnAyPRXwMhywzIzs2Ypkghq9xC4MDfvghJiMTOzHlAkEXw6Ik4vOxAzM+sZRa4j2KPrVczMrLcqUiPYSNKJ9TMj4qIS4jEzsyYrkgjagP5kncRmZtZiiiSCFyLijNIjMTOzHlGkj+DO0qMwM7MeUyQR/FLSgNqEpIGSxpQYk5mZNVGRRPATYEluekmaZ2ZmLaBIIlD+PsIRsZxifQtmZtYLFEkEsyWdIGnt9PgaMLvswMzMrDmKJIJjgI8B84F5wBhgfJlBmZlZ8xS5Mc1LZMNQm5lZCypyY5otJP1O0mNpehtJ3yk/NDMza4YiTUNXAqcB7wBExCO4hmBm1jKKJIL1IuKBunlLywjGzMyar0gieEXSZkAASNobeL7UqMzMrGmK3rP4CuCDkuYDzwAHlRqVmZk1TZGzhmYDu0h6D9AnIhaXH5aZmTVLl4mg/l4EUjYate9HYGbWGor0EQzIPU7KPTczsxZQpGno+7XnkvbMT5uZWe9XpGloA7K7k21ffjhmZtZsRc4amgEsB+YCx5UbjpmZNVuRpqGRzQjEzMx6xkqfNVTjs4bMzFpDkaah7wLPApNLjsXMzHpAkUSwGdmgczsDZ0TEXeWGZGZmzdTldQQR8WpEnEw24ug+km6XtEORwiWNlfSkpFmSTu1gnX0lzZT0uKSfr1z4Zma2uor0EfyKNOAc2WmkI4D7gLYutmsDJgKfIbuz2TRJUyJiZm6dUWS1jZ0iYqGkjVZpL8zMbJUVaRq6YBXL3hGYlcYqQtKNwDhgZm6do4CJEbEQ/nU3NDMza6IiieDRVSx7Y7JrD2pq9zvO2wJA0p/JahinR8Ttq/h6Zma2CookgufJblyv3LwANu2m1x8FfAoYBvxR0ocj4rX8SpLGA+MBRowY0Q0va2ZmNUUSwcyIWJXhJeYDw3PTw9K8vHnA/RHxDvCMpKfIEsO0/EoRcQXZPREYPXp0YGZm3abI6KODJI1LZwBtI6lI8oDsx3yUpJGS+pKddTSlbp1byWoDSBpM1lQ0u2D5ZmbWDYr8qN8D7AX0A4YCm0g6KiJ+09lGEbFU0nHAHWTt/5Mi4nFJZwDTI2JKWvZZSTOBZcDJEbFgNfbHzMxWUpGxhg7PT0vanOxIvtNEkLadCkytmzch9zyAE9PDzMx6QJGmoXeJiFlk1waYmVkL6DIRSBomabKklyW9JOkWuriYzMzMeo8iNYKryTp530/WR/CrNM/MzFpAkUQwJCKujoil6XENMKTkuMzMrEmKJIIFkg6S1JYeBwE+s8fMrEUUSQRfBvYFXiC7ynhv4PBOtzAzs16jyOmjzwJ75OdJWre0iMzMrKmKnDU0oW56F+qGgDAzs96rSNPQ+yT9RNJgSdcC3yQbTtrMzFpAkTuUfQV4jmxI6b9ExGdr9xgwM7Per8gdyr4IPA7cBRwk6SWAiPhlybGZmVkTFBl0bvf095X02J3sfgROBGZmLWClB50zM7PWstKDzpmZWWtxIjAzqzgnAjOziityQdkgSRdLmp4eF0oa1IzgzMysfEVqBJOARWTjDe2bnnsYajOzFlHk9NHNImKv3PT3JT1UUjxmZtZkRWoEb0j6eG1C0k7AG+WFZGZmzVSkRnAscG2uX2AhcGh5IZmZWTMVuaDsIWBbSQPT9KKygzIzs+YpctbQbyFLAE4CZmatp9A9i0uPwszMekyRRBClR2FmZj2mSGfxtpIWASJLCgIiIgaWGpmZmTVFkc7itmYEYmZmPaNIZ7EkHSTpu2l6uKQdyw/NzMyaoUgfwY+BjwJfStNLgImlRWRmZk1VpI9gTER8RNKDABGxUFLfkuMyM7MmKVIjeEdSG+nsIUlDgOWlRmVmZk1TJBH8CJgMbCTpLOBPwNmlRmVmZk1T5Kyhn0maAexMduronhHxROmRmZlZUxTpIyAi/gr8tTYt6TBgBHB3RPypnNDMzKwZukwEkn7Pu68uFrAdMA74ezlhmZlZsxSpEZxUNy3gyoj4YwnxmJlZkxXpI5hRP0/S4nLCMTOzZivSNDShfhZZ/0CXJI0FLgHagKsi4pwO1tsLuBnYISKmFynbzMy6R5GmodcbzFvW1Ubp2oOJwGeAecA0SVMiYmbdegOArwH3F4jFzMy6WZGmoQvr50kaV6DsHYFZETE7bXMjWQfzzLr1fgCcC5xcoEwzM+tmRZqGvthg9oYFyt4YmJubngeMqSv7I8DwiLhNkhOBmVkPKNI0tHuDeQ+s7gtL6gNcBBxWYN3xwHiAESMKdU+YmVlBRZqGDl/FsucDw3PTw9K8mgHA1sAfJAG8D5giaY/6DuOIuAK4AmD06NG+Y5qZWTfqcKwhSUekv8MkTZb0UnrcImlYgbKnAaMkjUyjle4PTKktjIh/RMTgiGiPiHbgPmCFJGBmZuXqbNC5Y9Pfq8l+wIemx6+ASV0VHBFLgeOAO4AngJsi4nFJZ0jaY7WiNjOzbtNZ09BbktYB3hsRV+fmXyPp60UKj4ipwNS6efXXJdTmf6pImWZm1r06qxHcCpwKvJRuVdmWHgcCvrLYzKxFdJYILiS7IngkcB3wFvAKcAhwRPmhmZlZM3TYNBQRy4EJ6WFmZi2qyzuUSRok6SJJ09PjQkmDmhGcmZmVr8itKieR9Qnsmx6LyM4kMjOzFlDkyuLNImKv3PT3JT1UUjxmZtZkRWoEb0j6eG1C0k7AG+WFZGZmzVSkRnAMcF2uX2AhcGh5IZmZWTMVGWvoYWBbSQPT9KLSozIzs6YpUiMAnADMzFpVkT4CMzNrYU4EZmYVV+SCsv9tRiBmZtYzitQIVHoUZmbWY4p0Fn9A0iO5aQEREduUFJOZmTVRkUTwDI3vW2xmZi2gSCJ4OyKeLT0SMzPrEUX6CI4vPQozM+sxRRLBo5Iu9jDUZmatqegw1IvwMNRmZi3Jw1CbmVWch6E2M6s4D0NtZlZxHobazKziPAy1mVnFefRRM7OKcyIwM6u4LpuGJB3SaH5EXNf94ZiZWbMVqRFcAIwGdgDOT39HlxmUmZk1T5HO4vkRcQKApF2AUyLin+WGZWZmzVKkRrC2pO0lfRJYF7hT0gdLjsvMzJqkSI3gFOBKYClwMPAccA3wifLCMjOzZilyQdltwG35eamJyMzMWkCRs4ZO7GDRRd0ci5mZ9YAifQQnAwMaPMzMrAUU6SN4PiK+X3okZmbWI4okgk0l3Qq8SdZR/OeIuKXUqMzMrGmKJIJxQBvQDxgKHCnpExHxtVIjMzOzpuiyjyAi7omIuyPitoi4EtgNGFykcEljJT0paZakUxssP1HSTEmPSPqdpE1WfhfMzGx1FBp0TtJ7Je0maTdgw4g4sMA2bcBEYFdgS+AASVvWrfYgMDoitgFuBs5bqejNzGy1dZkIJO0LPADsQ3bz+vsl7V2g7B2BWRExOyLeBm4ka2b6l4j4fW64ivuAYSsTvJmZrb4ifQTfBnaIiJcAJA0B7iI7gu/MxsDc3PQ8YEwn6x8B/KbRAknjgfEAI0aMKBCymZkVVaRpqE8tCSQLCm5XmKSDyEY0Pb/R8oi4IiJGR8ToIUOGdOdLm5lVXpEawe2S7gBuSNP70cGRe535wPDc9LA0713ScBXfBj4ZEW8VKNfMzLpRkbGGTpb0ReDjadYVETG5QNnTgFGSRpIlgP2BL+VXkLQ9cDkwtq7WYWZmTVLo5vUR8Uvgl7XpdPbQBmny+oiIBtsslXQccAfZdQiTIuJxSWcA0yNiCllTUH/gvyUB/D0i9lidHTIzs5XTYSKQNKGT7Y4hO5IHELBCIgCIiKnA1Lp5E3LPPYqpmVkP66xGMB64uINlyzz+kJlZa+gsEbwcERc2WpDO8jEzsxbQWSJYW9Iw4G1gcUS8kVvWsCnIzMx6n646i6cCfYEBkvoDTwF/AdYvOS4zM2uSDhNBRGydn5bUB9iU7DqCdkmHpEUNzxoyM7PeodDpowARsRyYBZwlaQEwkqyJqMOzhszMbM1XOBHkRcRl3R2ImZn1jG4dM8jMzHofJwIzs4pzIjAzqzgnAjOzinMiMDOrOCcCM7OKcyIwM6s4JwIzs4pzIjAzq7hVurLYzMrRfuptPR2CrcHmnPOFUsp1jcDMrOKcCMzMKs6JwMys4pwIzMwqzonAzKzinAjMzCrOicDMrOKcCMzMKs6JwMys4pwIzMwqzonAzKzinAjMzCrOicDMrOKcCMzMKs6JwMys4pwIzMwqzonAzKzinAjMzCrOicDMrOKcCMzMKq7URCBprKQnJc2SdGqD5etI+kVafr+k9jLjMTOzFZWWCCS1AROBXYEtgQMkbVm32hHAwojYHLgYOLeseMzMrLEyawQ7ArMiYnZEvA3cCIyrW2cccG16fjOwsySVGJOZmdVZq8SyNwbm5qbnAWM6Wicilkr6B7Ah8Ep+JUnjgfFpcomkJ0uJuHoGU/deV5lcH10T+Tuas5rf0U06WlBmIug2EXEFcEVPx9FqJE2PiNE9HYdZR/wdbY4ym4bmA8Nz08PSvIbrSFoLGAQsKDEmMzOrU2YimAaMkjRSUl9gf2BK3TpTgEPT872BuyMiSozJzMzqlNY0lNr8jwPuANqASRHxuKQzgOkRMQX4L+B6SbOAV8mShTWPm9tsTefvaBPIB+BmZtXmK4vNzCrOicDMrOKcCMysaSStL+krq7jtMZIO6e6YzImgciQdJmnoKm47VNLN3R2TVcr6wColgoi4LCKu695wDJwIqugwYJUSQUQ8FxF7d284VjHnAJtJekjS+enxmKRHJe0HIOkSSRPS889J+qOkPpJOl3RSmr+5pLskPSzpfyVt1oP71Os5EfQiktolPSHpSkmPS/qtpH6StpN0n6RHJE2W9G8dbL83MBr4WfpH7CdpZ0kPpn/ESWlE2B1SWetKek96ra3T6z+WymqTdEH6J35E0vHNfC+s1zoVeDoitgPuA7YDtgV2Ac6X9H7gNGA/SZ8GfgQcHhHL68r5GTAxIrYFPgY835zwW5MTQe8ziuwfYCvgNWAv4DrglIjYBngU+F6jDSPiZmA6cGD6RwzgGmC/iPgw2XUlx0bENLKL/c4EzgN+GhGP1RU3HmgHtkuv+7Pu20WriI8DN0TEsoh4EbgH2CEi/gkcBdwJXBoRT+c3kjQA2DgiJgNExJtpG1tFTgS9zzMR8VB6PgPYDFg/Iu5J864FPlGwrA+k8p5qsO0ZwGfIahDnNdh2F+DyiFgKEBGvrsxOmHXhw2TDzaxSM6atHCeC3uet3PNlZJ1vZdgQ6A8MANYt6TWsehaTfacA/oesCahN0hCyg5AHJG0C/CewPbCrpHeNWhwRi4F5kvaEf93gar1m7UArciLo/f4BLJT0H2n6YLIqdkfy/4hPAu2SNm+w7eXAd8mafBoNfnsncHQaLBBJG6zyHlhlRMQC4M+pr+mjwCPAw8DdwDeBF8mGnjkpIp4ju3nVVZLqD0YOBk6Q9AhwL/C+Ju1CS/IQE71IupXnryNi6zR9EtlR+63AZcB6wGyyzrWFHZSxF3A28AbZP+LHgAvI+gemAccC+wHjImKvdKe5e8k68GbXXj8lgPOAscA7wJURcWkJu21mJXMiMDOrODcNmZlVXK+4Q5mtPEkTgZ3qZl8SEVf3RDxmtuZy05CZWcW5acjMrOKcCMzMKs6JwMys4pwIzMwq7v8DE5VGxcp7V8kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Распределение целевого класса\")\n",
    "plt.ylabel(\"Доля от общего количества\")\n",
    "plt.bar(['no_toxic','toxic'], [1 - df['toxic'].sum() / len(df), df['toxic'].sum() / len(df)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имеет место дисбаланс целевого класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at unitary/toxic-bert were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/886208144.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'features.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'features.pickle'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/886208144.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unitary/toxic-bert'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unitary/toxic-bert'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     tokenized = df['text'].apply(\n\u001b[0m\u001b[1;32m      8\u001b[0m     lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=512, padding = 'max_length', truncation=True))\n\u001b[1;32m      9\u001b[0m     \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4136\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4137\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4138\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31/886208144.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unitary/toxic-bert'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     tokenized = df['text'].apply(\n\u001b[0;32m----> 8\u001b[0;31m     lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=512, padding = 'max_length', truncation=True))\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2165\u001b[0m                 ``convert_tokens_to_ids`` method).\n\u001b[1;32m   2166\u001b[0m         \"\"\"\n\u001b[0;32m-> 2167\u001b[0;31m         encoded_inputs = self.encode_plus(\n\u001b[0m\u001b[1;32m   2168\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2169\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2492\u001b[0m         )\n\u001b[1;32m   2493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2494\u001b[0;31m         return self._encode_plus(\n\u001b[0m\u001b[1;32m   2495\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2496\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m             )\n\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m         \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m         \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0mtokenized_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m                 \u001b[0mtokenized_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;31m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36m_tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0msplit_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_basic_tokenize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnever_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0;31m# If the token is part of the never_split set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    410\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip_accents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                     \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_strip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0msplit_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_split_on_punc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnever_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0moutput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhitespace_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36m_run_split_on_punc\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0mchar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_is_punctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open('features.pickle', 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "except:\n",
    "    tokenizer = BertTokenizer.from_pretrained('unitary/toxic-bert')\n",
    "    model = AutoModel.from_pretrained('unitary/toxic-bert').to(device)\n",
    "    tokenized = df['text'].apply(\n",
    "    lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=512, padding = 'max_length', truncation=True))\n",
    "    tokenized = np.array([i for i in tokenized.values])\n",
    "    display(tokenized.shape)\n",
    "    attention_mask = np.where(tokenized != 0, 1, 0)\n",
    "    batch_size = 100\n",
    "    mod = tokenized.shape[0] % batch_size\n",
    "    embeddings = []\n",
    "    for i in notebook.tqdm(range(tokenized.shape[0] // batch_size)):\n",
    "        batch = torch.LongTensor(tokenized[batch_size*i:batch_size*(i+1)]).cuda()\n",
    "        attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)]).cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "\n",
    "        embeddings.append(batch_embeddings[0][:,0,:].cpu().numpy())\n",
    "\n",
    "    batch = torch.LongTensor(tokenized[tokenized.shape[0] - mod:]).cuda()\n",
    "    attention_mask_batch = torch.LongTensor(attention_mask[tokenized.shape[0] - mod:]).cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "\n",
    "    embeddings.append(batch_embeddings[0][:,0,:].cpu().numpy())\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('features.pickle', 'wb') as f:\n",
    "    pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.concatenate(embeddings)\n",
    "target = df['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_valid, target_train, target_valid = train_test_split(\n",
    "    features,\n",
    "    target,\n",
    "    stratify=target,\n",
    "    random_state = 42,\n",
    "    test_size=0.4\n",
    ")\n",
    "features_valid, features_test, target_valid, target_test = train_test_split(\n",
    "    features_valid,\n",
    "    target_valid,\n",
    "    stratify=target_valid,\n",
    "    random_state = 42,\n",
    "    test_size=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наивная модель (константное значение)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DummyClassifier(strategy = 'constant', constant = 1)\n",
    "model.fit(features_train, target_train)\n",
    "preds = model.predict(features_test)\n",
    "accuracy_score(target_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state = 42)\n",
    "model.fit(features_train, target_train)\n",
    "preds = model.predict(features_valid)\n",
    "f'F1_valid = {f1_score(target_valid, preds)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_dataset = Pool(data=features_train,\n",
    "                  label=target_train)\n",
    "\n",
    "model = CatBoostClassifier(iterations=1000,\n",
    "                           depth=5,\n",
    "                           learning_rate=0.01,\n",
    "                           verbose=100,\n",
    "                           eval_metric = \"TotalF1\"\n",
    "                          )\n",
    "model.fit(cv_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(features_valid)\n",
    "f1_score(target_valid, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Случайный лес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state = 42, class_weight='balanced')\n",
    "\n",
    "grid_space={'max_depth':[3,5,10],\n",
    "              'n_estimators':[50]\n",
    "           }\n",
    "grid = GridSearchCV(model,param_grid=grid_space,cv=4,scoring='f1', verbose=2, n_jobs=-1)\n",
    "model_grid = grid.fit(np.concatenate([features_train, features_valid]), np.concatenate([target_train, target_valid]))\n",
    "display(f'Best hyperparameters are: {model_grid.best_params_}')\n",
    "display(f'Best estimator is: {model_grid.best_estimator_}')\n",
    "display(f'Best score is: {model_grid.best_score_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Полносвязная нейронная сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = MLPClassifier(hidden_layer_sizes=(64,64,64), learning_rate_init=0.001, max_iter=200, verbose=True, random_state = 42)\n",
    "model.fit(features_train, target_train)\n",
    "preds = model.predict(features_valid)\n",
    "f1_score(target_valid, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование лучшей модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state = 42)\n",
    "model.fit(features_train, target_train)\n",
    "preds = model.predict(features_test)\n",
    "f1_score(target_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате векторизации текстов с помощью BERT toxic получены эмбеддинги комментариев. Эмбеддинги использовались в качестве признаков при обучении логистической регрессии, случайного леса, модели catboost и полносвязной нейронной сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Итоги\n",
    "\n",
    "- Загружен и проанализирован датасет из текстов комментариев, проанализирован дисбаланс целевого класса\n",
    "- Исходные тексты были переведены в пространство векторов токенизатором BERT и предобработанны моделью BERT, чтобы достичь лучшей связи между векторами\n",
    "- Обучены модели: логистическая регрессия, случайный лес, catboost, полносвязная нейронная сеть. Их метрика F1 составила 0.945, 0.944, 0.94, 0.935 соответственно. Все из них прошли проверку на адекватность.\n",
    "- По лучшему значению метрики F1 на валидационной выборке была выбрана модель - логистическая регрессия.\n",
    "- Лучшая модель была протестирована на тестовой выборке с результатом F1 = 0.943, что удовлетворяет условию задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#Наивная-модель-(наиболее-часто-встречающееся-значение)\" data-toc-modified-id=\"Наивная-модель-(наиболее-часто-встречающееся-значение)-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Наивная модель (наиболее часто встречающееся значение)</a></span></li><li><span><a href=\"#Логистическая-регрессия\" data-toc-modified-id=\"Логистическая-регрессия-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Логистическая регрессия</a></span></li><li><span><a href=\"#Catboost-classifier\" data-toc-modified-id=\"Catboost-classifier-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Catboost classifier</a></span></li><li><span><a href=\"#Случайный-лес\" data-toc-modified-id=\"Случайный-лес-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Случайный лес</a></span></li><li><span><a href=\"#Полносвязная-нейронная-сеть\" data-toc-modified-id=\"Полносвязная-нейронная-сеть-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Полносвязная нейронная сеть</a></span></li></ul></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Итоги\" data-toc-modified-id=\"Итоги-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Итоги</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 9433,
    "start_time": "2023-06-13T05:46:47.642Z"
   },
   {
    "duration": 4,
    "start_time": "2023-06-13T05:46:57.078Z"
   },
   {
    "duration": 3706,
    "start_time": "2023-06-13T05:46:57.084Z"
   },
   {
    "duration": 253,
    "start_time": "2023-06-13T05:47:00.792Z"
   },
   {
    "duration": 117,
    "start_time": "2023-06-13T05:47:01.047Z"
   },
   {
    "duration": 71705,
    "start_time": "2023-06-13T05:47:01.166Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-13T05:48:12.877Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-13T05:48:12.878Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-13T05:48:12.880Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-13T05:48:12.882Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-13T05:48:12.883Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-13T05:48:12.884Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-13T05:48:12.886Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-13T05:48:12.888Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-13T05:48:12.889Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-13T05:48:12.891Z"
   },
   {
    "duration": 4,
    "start_time": "2023-06-13T05:56:11.246Z"
   },
   {
    "duration": 8,
    "start_time": "2023-06-13T05:56:11.252Z"
   },
   {
    "duration": 891,
    "start_time": "2023-06-13T05:56:11.261Z"
   },
   {
    "duration": 230,
    "start_time": "2023-06-13T05:56:12.154Z"
   },
   {
    "duration": 111,
    "start_time": "2023-06-13T05:56:12.387Z"
   },
   {
    "duration": 23680,
    "start_time": "2023-06-13T05:56:12.501Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-13T05:56:36.183Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-13T05:56:36.184Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-13T05:56:36.186Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-13T05:56:36.187Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-13T05:56:36.188Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-13T05:56:36.189Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-13T05:56:36.191Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-13T05:56:36.192Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-13T05:56:36.193Z"
   },
   {
    "duration": 0,
    "start_time": "2023-06-13T05:56:36.194Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
